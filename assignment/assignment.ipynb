{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# HW 2: Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q1.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "<br><br>\n",
        "When cleaning the price variable, I first chose to coerce the data all to a numeric type, then I created a new variable called 'Price_nan' which stores all the missing prices, then I turned all the missing price data to np.nan. I did this because if I chose a value like 0,1, or the median it would skew the mean and change the histogram. The total number of missing prices is 181.\n",
        "<br><br>\n",
        "2. Categorical variable: For the Minnesota police use of for data, `./data/mn_police_use_of_force.csv`, clean the `subject_injury` variable, handling the NA's; this gives a value `Yes` when a person was injured by police, and `No` when no injury occurred. What proportion of the values are missing? Is this a concern? Cross-tabulate your cleaned `subject_injury` variable with the `force_type` variable. Are there any patterns regarding when the data are missing?\n",
        "<br><br>\n",
        "When clenaing the 'subject_injury' variable, I created an 'empty_subject_injury' variable which stored all the missing values in 'subject_injury'. I then replaced all the missing values in 'subject_injury' with np.nan as this is the best approach when working with yes/no qualitative data. I then printed the values and found that there are 9848 missing values compared to 3077 reported values. This is very concerning, as this means that the vast majority of police subjects in Minnesota are unknown whether they were injured or not which shows that Minnesota police may not be lawfully arresting citizens with reasonable force. Lastly, I cross tabulated both 'subject_injury' and 'empty_subject_injury' and saw that over 7000 bodily force types were not reported, which is concerning because bodily force type was the force type that resulted in the most injuries (1286 injuries)!\n",
        "<br><br>\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "<br><br>\n",
        "For this variable, 1 = true, 0 = false, and 9 = missing. So I replaced all the 9's with np.nan, there are 31 missing values.\n",
        "<br><br>\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)\n",
        "<br><br>\n",
        "For cleaning the `ImposedSentenceAllChargeInContactEvent` variable I saw that it was a very messy string of arrays, so I coerced it to numeric and replaced empty values with np.nan. Then I looked at the min and max values and plotted the distribution and saw that their were obvious outliers for sentence length, so I did a log transformation to get rid of outliers while not deleting any data. I then plotted the distribution again and it looked a lot better. I then looked at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable and saw that it consisted of 5 categories: 0 = no sentence, 1 = jail/short-term, 2 = probation/community supervision, 4 = prison/long-term, 9 = missing. I then replaced the 9's with np.nan and renamed the other 4 variables as category types. I then printed out the clean values, and cross tabulated with the previous variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f1c8bb0",
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Question 1 Part 1'''\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/Users/hanimoudarres/Downloads/Foundations of ML/HW 2/wrangling/assignment/data/airbnb_hw.csv',low_memory=False)\n",
        "\n",
        "var = 'Price'\n",
        "\n",
        "# Initial description before coercion\n",
        "print('Before coercion: \\n', df[var].describe(),'\\n')\n",
        "\n",
        "# Coerce to numeric and create missing indicator\n",
        "df[var] = pd.to_numeric(df[var], errors='coerce')\n",
        "df[\"Price_nan\"] = df[var].isnull()\n",
        "\n",
        "# Description after coercion\n",
        "print('After coercion: \\n', df[var].describe(),'\\n')\n",
        "\n",
        "# Total missing values: 181\n",
        "print('Total Missings: \\n', sum(df['Price_nan']),'\\n')\n",
        "\n",
        "# Replace spaces with NaN\n",
        "df[var] = df[var].replace(' ',np.nan)\n",
        "\n",
        "# Plotting the distribution\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(df[var].dropna(), bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Price after Coercion', fontsize=16)\n",
        "plt.xlabel('Price', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "'''Question 1 Part 2'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('/Users/hanimoudarres/Downloads/Foundations of ML/HW 2/wrangling/assignment/data/mn_police_use_of_force.csv',low_memory=False)\n",
        "\n",
        "var = 'subject_injury'\n",
        "\n",
        "# Initial unique values\n",
        "print(df[var].unique(), '\\n')\n",
        "\n",
        "# Create missing indicator\n",
        "df['empty_subject_injury'] = df[var].isnull()\n",
        "\n",
        "# Replace empty strings with NaN\n",
        "df[var] = df[var].replace('', np.nan)\n",
        "\n",
        "# Value counts after replacement\n",
        "print(df[var].value_counts(), '\\n') \n",
        "\n",
        "# Total missing values\n",
        "# The vast majority of entries are missing!!\n",
        "print('Total Missings: \\n', sum(df['empty_subject_injury']), '\\n')\n",
        "\n",
        "# Cross Tabulation\n",
        "print(pd.crosstab(df['subject_injury'], df['force_type']))\n",
        "print (pd.crosstab(df['empty_subject_injury'], df['force_type']))\n",
        "print('\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Question 1 Part 3\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_parquet('/Users/hanimoudarres/Downloads/Foundations of ML/HW 2/wrangling/assignment/data/justice_data.parquet', engine='pyarrow')\n",
        "\n",
        "var = 'WhetherDefendantWasReleasedPretrial'\n",
        "\n",
        "# Initial unique values\n",
        "# 0 = false, 1 = true, 9 = missing\n",
        "print(df[var].unique(), '\\n')\n",
        "\n",
        "# Create missing indicator\n",
        "df['empty_release_pretrial'] = df[var] == 9\n",
        "\n",
        "# Replace empty strings with NaN\n",
        "df[var] = df[var].replace(9, np.nan)\n",
        "\n",
        "# Value counts after replacement\n",
        "print(df[var].value_counts(), '\\n') \n",
        "\n",
        "# Total missing values\n",
        "print('Total Missings: \\n', sum(df['empty_release_pretrial']), '\\n')\n",
        "\n",
        "\n",
        "\"\"\"Question 1 Part 4\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_parquet('/Users/hanimoudarres/Downloads/Foundations of ML/HW 2/wrangling/assignment/data/justice_data.parquet', engine='pyarrow')\n",
        "\n",
        "var = 'ImposedSentenceAllChargeInContactEvent'\n",
        "\n",
        "# Initial unique values\n",
        "# Shows mixed type numerical data that is very messy\n",
        "print(df[var].unique(), '\\n')\n",
        "\n",
        "# print value counts\n",
        "print(df[var].value_counts(dropna=False), '\\n')\n",
        "\n",
        "# Replace empty strings with NaN\n",
        "df[var] = df[var].replace('', np.nan)\n",
        "\n",
        "# Coerce to numeric\n",
        "df[var] = pd.to_numeric(df[var], errors='coerce')\n",
        "\n",
        "# Description after coercion\n",
        "print(df[var].describe(), '\\n')\n",
        "print(df[var].value_counts().head(10), '\\n')\n",
        "\n",
        "# min and max values\n",
        "print(df[var].min(), '\\n')\n",
        "print(df[var].max(), '\\n')\n",
        "\n",
        "# Plotting the distribution\n",
        "# Not a very good distribution due to many extreme outliers\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(df[var].dropna(), bins=50, color='lightgreen', edgecolor='black')\n",
        "plt.title('Distribution of Imposed Sentence After Coercion', fontsize=16)\n",
        "plt.xlabel('Imposed Sentence (Days)', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "# Log Transformation\n",
        "df[var + '_log'] = np.log1p(df[var])\n",
        "\n",
        "# Plotting the log-transformed distribution\n",
        "# Much better distribution after log transformation\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(df[var + '_log'].dropna(), bins=50, color='salmon', edgecolor='black')\n",
        "plt.title('Log-Transformed Distribution of Imposed Sentence', fontsize=16)\n",
        "plt.xlabel('Log(Imposed Sentence + 1)', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "var2 = 'SentenceTypeAllChargesAtConvictionInContactEvent'\n",
        "\n",
        "# Initial description\n",
        "# 0 = no sentence, 1 = jail/short-term, 2 = probation/community supervision, 4 = prison/long-term, 9 = missing\n",
        "print(df[var2].unique(), '\\n')\n",
        "\n",
        "# Value counts\n",
        "print(df[var2].value_counts(dropna=False), '\\n')\n",
        "\n",
        "# Replace 9 with NaN\n",
        "df[var2] = df[var2].replace(9, np.nan)\n",
        "\n",
        "# Convert to categorical\n",
        "df[var2] = df[var2].astype('category')\n",
        "\n",
        "df[var2] = df[var2].cat.rename_categories({\n",
        "    0: 'No Sentence',\n",
        "    1: 'Jail/Short-term',\n",
        "    2: 'Probation/Community Supervision',\n",
        "    4: 'Prison/Long-term'\n",
        "})\n",
        "\n",
        "# Value counts after replacement\n",
        "print(df[var2].value_counts(normalize=False), '\\n')\n",
        "\n",
        "# Cross Tabulation\n",
        "print(pd.crosstab(df[var], df[var2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a60a44e",
      "metadata": {
        "id": "5a60a44e"
      },
      "source": [
        "**Q2.** Go to https://sharkattackfile.net/ and download their dataset on shark attacks.\n",
        "\n",
        "1. Open the shark attack file using Pandas. It is probably not a csv file, so `read_csv` won't work.\n",
        "2. Drop any columns that do not contain data.\n",
        "3. Clean the year variable. Describe the range of values you see. Filter the rows to focus on attacks since 1940. Are attacks increasing, decreasing, or remaining constant over time?\n",
        "<br><br>\n",
        "The year variable ranges from 0 to 2026, however, it is unlikely that data has been collected since that long ago. After graphing the data from 1940 onward, it looks like shark attacks increased up until around 2017, and after that they significantly decreased.\n",
        "<br><br>\n",
        "4. Clean the Age variable and make a histogram of the ages of the victims.\n",
        "5. What proportion of victims are male?\n",
        "<br><br>\n",
        "~79%\n",
        "<br><br>\n",
        "6. Clean the `Type` variable so it only takes three values: Provoked and Unprovoked and Unknown. What proportion of attacks are unprovoked?\n",
        "<br><br>\n",
        "~74%\n",
        "<br><br>\n",
        "7. Clean the `Fatal Y/N` variable so it only takes three values: Y, N, and Unknown.\n",
        "8. Are sharks more likely to launch unprovoked attacks on men or women? Is the attack more or less likely to be fatal when the attack is provoked or unprovoked? Is it more or less likely to be fatal when the victim is male or female? How do you feel about sharks?\n",
        "<br><br>\n",
        "Sharks are slightly more likely to launch unprovoked attacks on women with ~85% of women attacks being unprovoked and ~77% of men attacks being unprovoked. When an attack is provoked it is slightly less likely to be fatal compared to an unprovoked attack with provoked attacks being ~2.4% fatal and unprovoked attacks being ~1.6% fatal. An attack is slightly more likely to be fatal when the victim is a male rather than a female with ~15% of male attacks being fatal and ~12% of female attacks being fatal. I feel that sharks are unbiased on which gender that they attack for the most part and it seems that most shark attacks aren't fatal regardless of whether the attack is provoked or not.\n",
        "<br><br>\n",
        "9. What proportion of attacks appear to be by white sharks? (Hint: `str.split()` makes a vector of text values into a list of lists, split by spaces.)\n",
        "<br><br>\n",
        "~7% of attacks appear to be by white sharks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40fb111e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" Question 2 \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\"\"\" Step 1 \"\"\"\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_excel('/Users/hanimoudarres/Downloads/Foundations of ML/HW 2/wrangling/assignment/data/shark_data.xls')\n",
        "\n",
        "\"\"\" Step 2 \"\"\"\n",
        "\n",
        "# Initial columns\n",
        "print(df.columns, len(df.columns))\n",
        "print('\\n')\n",
        "\n",
        "# Drop columns that are completely empty\n",
        "df = df.dropna(axis=1, how='all')\n",
        "\n",
        "# Columns after dropping empty ones\n",
        "# (it stayed the same in this case)\n",
        "print(df.columns, len(df.columns))\n",
        "print('\\n')\n",
        "\n",
        "\"\"\" Step 3 \"\"\"\n",
        "\n",
        "var = 'Year'\n",
        "\n",
        "# Coerce to numeric, forcing errors to NaN\n",
        "df[var] = pd.to_numeric(df[var], errors='coerce')\n",
        "\n",
        "# Look at range\n",
        "print(df[var].describe(), '\\n')\n",
        "\n",
        "# Filter for attacks since 1940\n",
        "df_recent = df[df[var] >= 1940]\n",
        "\n",
        "# Plot number of attacks per year since 1940\n",
        "attacks_per_year = df_recent[var].value_counts().sort_index()\n",
        "attacks_per_year.plot(figsize=(12,5), title=\"Shark attacks since 1940\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of attacks\")\n",
        "plt.show()\n",
        "\n",
        "\"\"\" Step 4 \"\"\"\n",
        "\n",
        "# Remove non-numeric characters and convert to float\n",
        "df_recent['Age'] = pd.to_numeric(df_recent['Age'], errors='coerce')\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10,5))\n",
        "df_recent['Age'].hist(bins=30)\n",
        "plt.title(\"Histogram of Victim Ages\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "\"\"\" Step 5 \"\"\"\n",
        "\n",
        "# Clean Gender variable\n",
        "df_recent['Sex'] = df_recent['Sex'].str.strip().str.upper()  # Remove whitespace, uppercase\n",
        "\n",
        "# Shows M, F, M X 2, and LLI --> need to fix\n",
        "print(df_recent['Sex'].value_counts(dropna=False), '\\n')\n",
        "\n",
        "sex_mapping = {\n",
        "    'M': 'M',\n",
        "    'F': 'F',\n",
        "    'M X 2': 'M',     # assume these are males\n",
        "    'LLI': 'Unknown'  # treat as unknown/missing\n",
        "}\n",
        "df_recent['Sex'] = df_recent['Sex'].str.strip().map(sex_mapping).fillna('Unknown')\n",
        "\n",
        "# Check distribution after cleaning\n",
        "print(df_recent['Sex'].value_counts(dropna=False), '\\n')\n",
        "\n",
        "# Compute proportion male\n",
        "male_prop = (df_recent['Sex'] == 'M').mean()\n",
        "print(f\"Proportion of male victims: {male_prop:.2f}\\n\")\n",
        "\n",
        "\n",
        "\"\"\" Step 6 \"\"\"\n",
        "\n",
        "# Standardize Type variable\n",
        "df_recent['Type'] = df_recent['Type'].str.strip().str.capitalize()\n",
        "\n",
        "# Map all types to Provoked, Unprovoked, Unknown\n",
        "type_mapping = {\n",
        "    'Unprovoked': 'Unprovoked',\n",
        "    'Provoked': 'Provoked',\n",
        "}\n",
        "df_recent['Type'] = df_recent['Type'].map(type_mapping).fillna('Unknown')\n",
        "\n",
        "# Proportion unprovoked\n",
        "unprovoked_prop = (df_recent['Type'] == 'Unprovoked').mean()\n",
        "print(f\"Proportion of unprovoked attacks: {unprovoked_prop:.2f}\\n\")\n",
        "\n",
        "\n",
        "\"\"\" Step 7 \"\"\"\n",
        "\n",
        "# Standardize Fatal variable\n",
        "df_recent['Fatal Y/N'] = df_recent['Fatal Y/N'].str.strip().str.upper()\n",
        "\n",
        "# Map to Y, N, Unknown\n",
        "fatal_mapping = {\n",
        "    'Y': 'Y', \n",
        "    'N': 'N'\n",
        "}\n",
        "\n",
        "df_recent['Fatal Y/N'] = df_recent['Fatal Y/N'].map(fatal_mapping).fillna('Unknown')\n",
        "\n",
        "# Check distribution\n",
        "print(df_recent['Fatal Y/N'].value_counts(normalize=False), '\\n')\n",
        "\n",
        "\"\"\" Step 8 \"\"\"\n",
        "\n",
        "# Are unprovoked attacks more likely on men or women?\n",
        "print(pd.crosstab(df_recent['Sex'], df_recent['Type'], normalize='index'))\n",
        "print('\\n')\n",
        "\n",
        "# Fatality by attack type\n",
        "print(pd.crosstab(df_recent['Type'], df_recent['Fatal Y/N'], normalize='index'))\n",
        "print('\\n')\n",
        "\n",
        "# Fatality by sex\n",
        "print(pd.crosstab(df_recent['Sex'], df_recent['Fatal Y/N'], normalize='index'))\n",
        "print('\\n')\n",
        "\n",
        "\"\"\" Step 9 \"\"\"\n",
        "\n",
        "# Make lowercase and split text into words\n",
        "df_recent['Species '] = df_recent['Species '].str.lower().fillna('')\n",
        "species_lists = df_recent['Species '].str.split()\n",
        "\n",
        "# Check for white shark\n",
        "white_shark_prop = species_lists.apply(lambda x: 'white' in x and 'shark' in x).mean()\n",
        "print(f\"Proportion of attacks by white sharks: {white_shark_prop:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q3.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  <br><br>\n",
        "  The paper introduces a framework for data tidying, a subset of data cleaning that structures datasets for easier manipulation and analysis. It promotes a standard where each variable is a column and each observation is a row, enabling the creation of \"tidy tools\" that work together seamlessly.\n",
        "  <br><br>\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  <br><br>\n",
        "  The tidy data standard aims to simplify the process of initial data exploration and tool development. By providing a standard way to organize values, it eliminates the need to reinvent the wheel for every new dataset and reduces the time spent \"munging\" data between different analysis tools.\n",
        "  <br><br>\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  <br><br>\n",
        "  Wickham compares datasets to families: tidy datasets are \"all alike\" because they follow a strict standard, while every messy dataset is disorganized in its own unique way. He notes that while identifying variables in a specific project is intuitive, providing a universal definition is difficult because the role of data (as a variable or a value) often depends on the specific goals of the analysis.\n",
        "  <br><br>\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  <br><br>\n",
        "  Wickham defines values as the individual numbers or strings within a dataset. A variable consists of all values measuring the same attribute across different units, while an observation contains all values measured on a single unit across different attributes.\n",
        "  <br><br>  \n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  <br><br>\n",
        "  Tidy data is a standard mapping of semantics to structure where each variable forms a column, each observation forms a row, and each type of observational unit forms its own table.\n",
        "  <br><br>  \n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  <br><br>\n",
        "  The five common problems are:<br>1. values in column headers<br>2. multiple variables in one column<br>3. variables in both rows and columns<br>4. multiple types per table<br>5. single types across multiple tables<br><br>Table 4 is messy because column headers represent income values rather than variable names. Melting is the process of turning these column headers into rows to create a \"molten\" dataset.\n",
        "  <br><br>  \n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  <br><br>\n",
        "  Table 11 is messy because variables (days) are spread across columns and the \"element\" column contains names of variables like 'tmin' and 'tmax'. Table 12(a) is molten because it stacks the days into rows, but it only becomes truly tidy in Table 12(b) after the 'element' values are \"cast\" into their own individual columns.\n",
        "  <br><br>  \n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?\n",
        "  <br><br>\n",
        "  The chicken-and-egg problem refers to the fact that tidy data is only useful if there are tools to handle it, but tools are only designed for tidy data. Wickham hopes future research will use human-computer interaction methods to improve the cognitive side of data analysis and develop tools that automatically optimize between different data storage formats."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
